<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">

<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="语言,编程,日常学习,TensorFlow,Python,">










<meta name="description" content="1.tensorflow基本介绍1.TensorFlow 简介​    TensorFlow是一个基于数据流编程的符号数学系统，被广泛应用于各类机器学习算法的编程实现，其前身是谷歌的神经网络算法库 2.TensorFlow基本术语张量（tensor）：​    张量就是基于向量和矩阵的推广，通俗点理解就是可以将标量看成零阶张量，向量看成一阶张量，矩阵就是二阶张量。 图（graph）​    代表着">
<meta name="keywords" content="语言,编程,日常学习,TensorFlow,Python">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow入门教程">
<meta property="og:url" content="http://yoursite.com/2018/05/27/1.tensorflow基本介绍/index.html">
<meta property="og:site_name" content="Byy">
<meta property="og:description" content="1.tensorflow基本介绍1.TensorFlow 简介​    TensorFlow是一个基于数据流编程的符号数学系统，被广泛应用于各类机器学习算法的编程实现，其前身是谷歌的神经网络算法库 2.TensorFlow基本术语张量（tensor）：​    张量就是基于向量和矩阵的推广，通俗点理解就是可以将标量看成零阶张量，向量看成一阶张量，矩阵就是二阶张量。 图（graph）​    代表着">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="c:%5CUsers%5CAdministrator%5CPictures%5C1529558773906.png">
<meta property="og:image" content="c:%5CUsers%5Chp%5CPictures%5Cequation.svg">
<meta property="og:updated_time" content="2019-09-11T11:31:44.846Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow入门教程">
<meta name="twitter:description" content="1.tensorflow基本介绍1.TensorFlow 简介​    TensorFlow是一个基于数据流编程的符号数学系统，被广泛应用于各类机器学习算法的编程实现，其前身是谷歌的神经网络算法库 2.TensorFlow基本术语张量（tensor）：​    张量就是基于向量和矩阵的推广，通俗点理解就是可以将标量看成零阶张量，向量看成一阶张量，矩阵就是二阶张量。 图（graph）​    代表着">
<meta name="twitter:image" content="c:%5CUsers%5CAdministrator%5CPictures%5C1529558773906.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/05/27/1.tensorflow基本介绍/">





  <title>Tensorflow入门教程 | Byy</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Byy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home //首页"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags //标签"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th //分类"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive //归档"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/1.tensorflow基本介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yang295513">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Byy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Tensorflow入门教程</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T13:47:40+08:00">
                2018-05-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1-tensorflow基本介绍"><a href="#1-tensorflow基本介绍" class="headerlink" title="1.tensorflow基本介绍"></a>1.tensorflow基本介绍</h1><h3 id="1-TensorFlow-简介"><a href="#1-TensorFlow-简介" class="headerlink" title="1.TensorFlow 简介"></a>1.TensorFlow 简介</h3><p>​    TensorFlow是一个基于数据流编程的符号数学系统，被广泛应用于各类机器学习算法的编程实现，其前身是谷歌的神经网络算法库</p>
<h3 id="2-TensorFlow基本术语"><a href="#2-TensorFlow基本术语" class="headerlink" title="2.TensorFlow基本术语"></a>2.TensorFlow基本术语</h3><h4 id="张量（tensor）："><a href="#张量（tensor）：" class="headerlink" title="张量（tensor）："></a>张量（tensor）：</h4><p>​    张量就是基于向量和矩阵的推广，通俗点理解就是可以将标量看成零阶张量，向量看成一阶张量，矩阵就是二阶张量。</p>
<h4 id="图（graph）"><a href="#图（graph）" class="headerlink" title="图（graph）"></a>图（graph）</h4><p>​    代表着一段内存地址，也可以理解成所有的节点和张量的集合</p>
<h4 id="节点（op）"><a href="#节点（op）" class="headerlink" title="节点（op）"></a>节点（op）</h4><p>​    每个运算和张量都是一个节点，每个节点就是一个op</p>
<h4 id="会话-Session"><a href="#会话-Session" class="headerlink" title="会话(Session)"></a>会话(Session)</h4><p>​    会话的作用就是执行图的计算，众所周知在TensorFlow中会话之前的都是图的定义或者是op的定义，只能表示关系不参与计算，所以需要用会话让图真正的执行起来</p>
<h1 id="2-张量（tensor）的使用以及注意事项"><a href="#2-张量（tensor）的使用以及注意事项" class="headerlink" title="2.张量（tensor）的使用以及注意事项"></a>2.张量（tensor）的使用以及注意事项</h1><h4 id="1-张量的基本概念"><a href="#1-张量的基本概念" class="headerlink" title="1.张量的基本概念"></a>1.张量的基本概念</h4><p>​    张量就是基于向量和矩阵的推广，通俗点理解就是可以将标量看成零阶张量，向量看成一阶张量，矩阵就是二阶张量。</p>
<h4 id="2-张量的数据类型"><a href="#2-张量的数据类型" class="headerlink" title="2.张量的数据类型"></a>2.张量的数据类型</h4><table>
<thead>
<tr>
<th align="center">数据类型</th>
<th align="center">Python类型</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>DT_FLOAT</strong></td>
<td align="center"><strong>tf.float32</strong></td>
<td align="center"><strong>32位浮点数</strong></td>
</tr>
<tr>
<td align="center">DT_DOUBLE</td>
<td align="center">tf.float64</td>
<td align="center">64位浮点数（精度和float32精度一致）</td>
</tr>
<tr>
<td align="center">DT_INT64</td>
<td align="center">tf.int64</td>
<td align="center">64位有符号整数</td>
</tr>
<tr>
<td align="center"><strong>DT_INT32</strong></td>
<td align="center"><strong>tf.int32</strong></td>
<td align="center"><strong>32位有符号整数（精度和int32精度一致）</strong></td>
</tr>
<tr>
<td align="center">DT_INT13</td>
<td align="center">tf.int16</td>
<td align="center">16位有符号整数</td>
</tr>
<tr>
<td align="center"><strong>DT_INT8</strong></td>
<td align="center"><strong>tf.int8</strong></td>
<td align="center"><strong>8位有符号整数</strong></td>
</tr>
<tr>
<td align="center">DT_STRING</td>
<td align="center">tf.string</td>
<td align="center">可变长度的字节数组，每一个张量元素都是一个字节数组。</td>
</tr>
<tr>
<td align="center">DT_BOOL</td>
<td align="center">tf.bool</td>
<td align="center">布尔型</td>
</tr>
<tr>
<td align="center">DT_COMPLEX64</td>
<td align="center">tf.compiex64</td>
<td align="center">由两个32位浮点数组组成的复数：实数和虚数</td>
</tr>
<tr>
<td align="center">DT_QINT32</td>
<td align="center">tf.qint32</td>
<td align="center">用于量化Ops的8位有符号整数</td>
</tr>
<tr>
<td align="center">DT.QINT8</td>
<td align="center">tf.qint8</td>
<td align="center">用于量化Ops的8位有符号整形</td>
</tr>
<tr>
<td align="center">DT_QUINT8</td>
<td align="center">tf.quint8</td>
<td align="center">用于量化Ops的8位无符号整形</td>
</tr>
</tbody></table>
<h4 id="3-张量的代码"><a href="#3-张量的代码" class="headerlink" title="3.张量的代码"></a>3.张量的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入tensorflow包，简写为tf</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个3.0的常量的张量</span></span><br><span class="line">tensor=tf.constant(<span class="number">3.0</span>)</span><br><span class="line"><span class="comment">#显示tensor的结果</span></span><br><span class="line">print(tensor)</span><br><span class="line"><span class="comment">#结果将会是：Tensor("Const:0", shape=(), dtype=float32)</span></span><br></pre></td></tr></table></figure>

<h5 id="其中Const表示进行的op操作名字"><a href="#其中Const表示进行的op操作名字" class="headerlink" title="其中Const表示进行的op操作名字"></a>其中Const表示进行的op操作名字</h5><h5 id="shape表示张量的维度-表示标量，（1）表示向量，（2，3）表示2行3列的张量"><a href="#shape表示张量的维度-表示标量，（1）表示向量，（2，3）表示2行3列的张量" class="headerlink" title="shape表示张量的维度,()表示标量，（1）表示向量，（2，3）表示2行3列的张量"></a>shape表示张量的维度,()表示标量，（1）表示向量，（2，3）表示2行3列的张量</h5><h5 id="dtype表示张量的数据类型"><a href="#dtype表示张量的数据类型" class="headerlink" title="dtype表示张量的数据类型"></a>dtype表示张量的数据类型</h5><h3 id="4-生成张量"><a href="#4-生成张量" class="headerlink" title="4.生成张量"></a>4.生成张量</h3><h5 id="创建一个常数张量"><a href="#创建一个常数张量" class="headerlink" title="创建一个常数张量"></a>创建一个常数张量</h5><p>tf.constant(value,dtype=None,shape=None,name=”Const”)</p>
<p>创建一个dtype类型的维度为shape的常数张量</p>
<h5 id="固定值初始化"><a href="#固定值初始化" class="headerlink" title="固定值初始化"></a>固定值初始化</h5><h6 id="tf-zeros-n-m-tf-dtype"><a href="#tf-zeros-n-m-tf-dtype" class="headerlink" title="tf.zeros([n,m],tf.dtype)"></a>tf.zeros([n,m],tf.dtype)</h6><p>获取一个n行m列的零元素tf.dtype类型的张量</p>
<h6 id="tf-ones-n-m-tf-dtype"><a href="#tf-ones-n-m-tf-dtype" class="headerlink" title="tf.ones([n,m],tf.dtype)"></a>tf.ones([n,m],tf.dtype)</h6><p>获取一个n行m列的1元素tf.dtype类型的张量</p>
<h5 id="随机值初始化"><a href="#随机值初始化" class="headerlink" title="随机值初始化"></a>随机值初始化</h5><h6 id="tf-random-normal-n-m-mean-2-0-stddev-4-seed-12"><a href="#tf-random-normal-n-m-mean-2-0-stddev-4-seed-12" class="headerlink" title="tf.random_normal([n,m],mean=2.0,stddev=4,seed=12)"></a>tf.random_normal([n,m],mean=2.0,stddev=4,seed=12)</h6><p>创建一个n行m列的<a href="[https://baike.baidu.com/item/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/829892?fr=aladdin](https://baike.baidu.com/item/正态分布/829892?fr=aladdin)">正态分布</a>（高斯分布）平均值为2.0，方差为4,随机种子为12张量</p>
<h3 id="5-占位符"><a href="#5-占位符" class="headerlink" title="5.占位符"></a>5.占位符</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder(dtype,shape,name)</span><br></pre></td></tr></table></figure>

<p>dtype张量的数据类型</p>
<p>shape张量的维度 [2,3]生成一个2行3列的占位符,[None,3]表示生成一个不确定行和3列的占位符</p>
<h3 id="6-张量的维度调整"><a href="#6-张量的维度调整" class="headerlink" title="6.张量的维度调整"></a>6.张量的维度调整</h3><h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><p>如果调整的过程中生成了新的张量，这种调整称作动态调整</p>
<h4 id="静态调整"><a href="#静态调整" class="headerlink" title="静态调整"></a>静态调整</h4><h5 id="语法-张量名字-set-shape-n-m-调整张量为n行m列"><a href="#语法-张量名字-set-shape-n-m-调整张量为n行m列" class="headerlink" title="语法: 张量名字.set_shape([n,m]) 调整张量为n行m列"></a>语法: 张量名字.set_shape([n,m]) 调整张量为n行m列</h5><ul>
<li>注意 静态张量只能调整之前不确定维度的张量，比如shape=[None,3]的张量</li>
</ul>
<h4 id="动态调整"><a href="#动态调整" class="headerlink" title="动态调整"></a>动态调整</h4><h5 id="语法-tf-reshape-tensor-shape-name-None"><a href="#语法-tf-reshape-tensor-shape-name-None" class="headerlink" title="语法: tf.reshape(tensor,shape,name=None)"></a>语法: tf.reshape(tensor,shape,name=None)</h5><p>动态张量会生成新的张量而且可以跨维度修改，也就是二阶张量可以向n阶张量改变，但是改变前和改变后其总个数必须一致，如果不知道具体维度，需要填写成-1</p>
<h3 id="7-改变张量的类型"><a href="#7-改变张量的类型" class="headerlink" title="7.改变张量的类型"></a>7.改变张量的类型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.cast(x,dtype,name=<span class="string">"None"</span>)</span><br></pre></td></tr></table></figure>

<p>将x张量转换为dtype类型的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.cast([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]],tf.float32)</span><br></pre></td></tr></table></figure>

<p>将列表从整形转换成float32类型</p>
<h3 id="8-张量的切片和扩展"><a href="#8-张量的切片和扩展" class="headerlink" title="8.张量的切片和扩展"></a>8.张量的切片和扩展</h3> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.concat(value,axis,name=<span class="string">"concat"</span>)</span><br></pre></td></tr></table></figure>

<p>可以将连个张量拼接起来</p>
<p>value 可以是个列表</p>
<p>axis 表示按行合并还是按列合并 0表示按行合并，1表示按列合并</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义两个列表</span></span><br><span class="line">a=[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">b=[[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment">#合并两个列表</span></span><br><span class="line">hangCat=tf.concat([a,b],axis=<span class="number">0</span>)</span><br><span class="line">lieCat=tf.concat([a,b],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#由于上面只是搭建了个图结果并没有实际运行,接下来进行实际运行。</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(<span class="string">"行合并"</span>)</span><br><span class="line">    print(sess.run(hangCat))</span><br><span class="line">    print(<span class="string">"列合并"</span>)</span><br><span class="line">    print(sess.run(lieCat))</span><br><span class="line">    </span><br><span class="line"><span class="comment">#运行结果为：</span></span><br><span class="line">行合并</span><br><span class="line">[[ <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>]</span><br><span class="line"> [ <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span>]</span><br><span class="line"> [<span class="number">10</span> <span class="number">11</span> <span class="number">12</span>]]</span><br><span class="line">列合并</span><br><span class="line">[[ <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span>]]</span><br></pre></td></tr></table></figure>

<h1 id="3-会话（Session）"><a href="#3-会话（Session）" class="headerlink" title="3.会话（Session）"></a>3.会话（Session）</h1><p>​    会话的作用就是执行图的计算，众所周知在TensorFlow中会话之前的都是图的定义或者是op的定义，只能表示关系不参与计算，所以需要用会话让图真正的运行起来</p>
<h3 id="基本写法"><a href="#基本写法" class="headerlink" title="基本写法"></a>基本写法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(fetches,feed_dict=<span class="literal">None</span>,options=<span class="literal">None</span>,run_metadata=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h3 id="1-run方法"><a href="#1-run方法" class="headerlink" title="1.run方法"></a>1.run方法</h3><h6 id="fetches-运行ops和计算tensor"><a href="#fetches-运行ops和计算tensor" class="headerlink" title="fetches 运行ops和计算tensor"></a>fetches 运行ops和计算tensor</h6><h6 id="feed-dict-可选项（字典类型），提取使用占位符之后给图提供数据"><a href="#feed-dict-可选项（字典类型），提取使用占位符之后给图提供数据" class="headerlink" title="feed_dict 可选项（字典类型），提取使用占位符之后给图提供数据"></a>feed_dict 可选项（字典类型），提取使用占位符之后给图提供数据</h6><h1 id="4-变量（Variable）"><a href="#4-变量（Variable）" class="headerlink" title="4.变量（Variable）"></a>4.变量（Variable）</h1><p>​    变量是一种特殊的张量，也是一种op,它能够被存储持久化，他们的值就是张量，默认被训练</p>
<h5 id="1-变量的定义"><a href="#1-变量的定义" class="headerlink" title="1.变量的定义"></a>1.变量的定义</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(initial_balue=<span class="literal">None</span>,name=<span class="literal">None</span>,trainable=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h6 id="initial-value-值-可以用正态分布或者固定生成张量的值"><a href="#initial-value-值-可以用正态分布或者固定生成张量的值" class="headerlink" title="initial_value 值 可以用正态分布或者固定生成张量的值"></a>initial_value 值 可以用正态分布或者固定生成张量的值</h6><ul>
<li>注意：使用变量的时候必须先运行全局初始化初始化 tf.global_variables_initializer()</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用正态分布分配变量的值</span></span><br><span class="line">var=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],mean=<span class="number">2.0</span>,stddev=<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#开启会话并运行初始化</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#运行初始化(运行全局初始化变量前变量var并未被真正赋值)</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(var))</span><br></pre></td></tr></table></figure>

<h1 id="4-TensorBoard-的使用和用法"><a href="#4-TensorBoard-的使用和用法" class="headerlink" title="4.TensorBoard 的使用和用法"></a>4.TensorBoard 的使用和用法</h1><h5 id="1-TensorBoard的简介"><a href="#1-TensorBoard的简介" class="headerlink" title="1.TensorBoard的简介"></a>1.TensorBoard的简介</h5><p>​    TensorBoard是Tensorflow的可视化工具，它可以通过Tensorflow程序运行过程中输出的日志文件可视化Tensorflow程序的运行状态。TensorBoard和Tensorflow程序跑在不同的进程中，TensorBoard会自动读取最新的TensorFlow日志文件，并呈现当前TensorFlow程序运行的最新状态。</p>
<h5 id="2-TensorBoard代码"><a href="#2-TensorBoard代码" class="headerlink" title="2.TensorBoard代码"></a>2.TensorBoard代码</h5><p>写入事务文件需要找到TensorFlow包里面的事务包summary里面的FileWriter方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.FileWriter(logdir,graph=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h6 id="logdir事务文件的绝对路径"><a href="#logdir事务文件的绝对路径" class="headerlink" title="logdir事务文件的绝对路径"></a>logdir事务文件的绝对路径</h6><h6 id="graph写出事务文件的图"><a href="#graph写出事务文件的图" class="headerlink" title="graph写出事务文件的图"></a>graph写出事务文件的图</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#收集变量</span></span><br><span class="line">tf.summary.scalar(name=<span class="string">""</span>,tensor)<span class="comment">#收集损失函数和准确率</span></span><br><span class="line">tf.summary.histogram(name=<span class="string">""</span>,tensor)<span class="comment">#收集高纬度的变量参数</span></span><br><span class="line">tf.summary.image(name=<span class="string">""</span>,tensor)<span class="comment">#收集输入的图片张量，能显示图片</span></span><br></pre></td></tr></table></figure>

<h6 id="name-表示TensorBoard里面显示的名称"><a href="#name-表示TensorBoard里面显示的名称" class="headerlink" title="name 表示TensorBoard里面显示的名称"></a>name 表示TensorBoard里面显示的名称</h6><h6 id="tensor表示要收集的张量"><a href="#tensor表示要收集的张量" class="headerlink" title="tensor表示要收集的张量"></a>tensor表示要收集的张量</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mergin=tf.summary.merge_all()<span class="comment">#收集所有的张量</span></span><br><span class="line">summary=sess.run(mergin)<span class="comment">#每次迭代都要运行的合并</span></span><br><span class="line">FileWriter.add_summary(summary,i)<span class="comment">#每次迭代都要添加到事务文件</span></span><br></pre></td></tr></table></figure>

<h5 id="3-TensorBoard的演示"><a href="#3-TensorBoard的演示" class="headerlink" title="3.TensorBoard的演示"></a>3.TensorBoard的演示</h5><p>​    首先代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#通过两个变量的相加演示tensorboard的用法</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义两个正态分布的随机变量</span></span><br><span class="line">var1=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],mean=<span class="number">2.0</span>,stddev=<span class="number">1.0</span>))</span><br><span class="line">var2=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],mean=<span class="number">3.0</span>,stddev=<span class="number">2.0</span>))</span><br><span class="line"><span class="comment">#定义加法op</span></span><br><span class="line">add=tf.add(var1,var2)</span><br><span class="line">init_variable=tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#开启会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#初始化变量</span></span><br><span class="line">    sess.run(init_variable)</span><br><span class="line">    <span class="comment">#导出事务文件</span></span><br><span class="line">    tf.summary.FileWriter(<span class="string">"./board"</span>,graph=sess.graph)</span><br><span class="line">    print(sess.run(add))</span><br></pre></td></tr></table></figure>

<p>​    然后启动命令行，打出下列命令</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir="D:\code\python\tensortflow\board"</span><br></pre></td></tr></table></figure>

<p>​    之后显示命令行下面显示如下信息</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TensorBoard <span class="number">1</span>.<span class="number">5</span>.<span class="number">1</span> <span class="built_in">at</span> http://By:<span class="number">6006</span> (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure>

<p>这就表示TensorBoard正常启动了，在浏览器输入（<a href="http://By:6006）就能正常访问tensorboard了" target="_blank" rel="noopener">http://By:6006）就能正常访问tensorboard了</a></p>
<h1 id="5-损失函数"><a href="#5-损失函数" class="headerlink" title="5.损失函数"></a>5.损失函数</h1><h3 id="1-均方误差"><a href="#1-均方误差" class="headerlink" title="1.均方误差"></a>1.均方误差</h3><p>​    计算方法是求预测值与真实值之间距离的平方和</p>
<p>​    公式如图所示：<img src="C:%5CUsers%5CAdministrator%5CPictures%5C1529558773906.png" alt></p>
<h1 id="6-梯度下降算法"><a href="#6-梯度下降算法" class="headerlink" title="6.梯度下降算法"></a>6.梯度下降算法</h1><h3 id="1-基本思想"><a href="#1-基本思想" class="headerlink" title="1.基本思想"></a>1.基本思想</h3><pre><code>梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。</code></pre><h3 id="2-代码"><a href="#2-代码" class="headerlink" title="2.代码"></a>2.代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.tarin.GradientDescentOptimizer(learning_rate)</span><br></pre></td></tr></table></figure>

<h6 id="learning-rate-学习率-通常填写0-0到1-0之间的浮点数"><a href="#learning-rate-学习率-通常填写0-0到1-0之间的浮点数" class="headerlink" title="learning_rate 学习率 通常填写0.0到1.0之间的浮点数"></a>learning_rate 学习率 通常填写0.0到1.0之间的浮点数</h6><h1 id="6-简单的线性回归案例"><a href="#6-简单的线性回归案例" class="headerlink" title="6.简单的线性回归案例"></a>6.简单的线性回归案例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#假设有一个函数关系式y=x*0.7+0.2  也就是y=x*w+b这个关系，若只知道y的结果和x的结果若干组，那么能否正确让w为0.7 b为0.2呢?</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.生成100组x的</span></span><br><span class="line">xDist=tf.random_normal([<span class="number">100</span>,<span class="number">1</span>],mean=<span class="number">0.5</span>,stddev=<span class="number">0.5</span>,name=<span class="string">"xDist"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成目标值y的结果</span></span><br><span class="line">y_true=tf.matmul(xDist,[[<span class="number">0.7</span>]])+<span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.建立线性回归模型</span></span><br><span class="line"><span class="comment">#因为权重和偏置是需要不断训练改变的，所有需要定义成变量</span></span><br><span class="line">w=tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">1</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>),name=<span class="string">"w"</span>)</span><br><span class="line">b=tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">1</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">0.0</span>),name=<span class="string">"b"</span>)</span><br><span class="line">y=tf.matmul(xDist,w)+b</span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数和使用梯度下降优化器优化，使用最小损失优化，学习率为0.1</span></span><br><span class="line">loss=tf.reduce_mean(tf.square(y_true-y))</span><br><span class="line">train_op=tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.定义全局变量全局初始化</span></span><br><span class="line">init_var=tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.开启会话开始训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#运行初始化变量</span></span><br><span class="line">    sess.run(init_var)</span><br><span class="line">    <span class="comment">#打印训练前权重和偏值,因为权重和偏置并没有被运行，所以需要使用eval方法实时获取权重和偏置</span></span><br><span class="line">    print(<span class="string">"训练前权重:%f权重:%f"</span> % (w.eval(),b.eval()))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">        sess.run(train_op)</span><br><span class="line">        print(<span class="string">"%d轮，权重为:%f,偏置为:%f"</span> % (i,w.eval(),b.eval()))</span><br></pre></td></tr></table></figure>

<h1 id="7-梯度爆炸"><a href="#7-梯度爆炸" class="headerlink" title="7.梯度爆炸"></a>7.梯度爆炸</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#假设有一个函数关系式y=x*0.7+0.2  也就是y=x*w+b这个关系，若只知道y的结果和x的结果若干组，那么能否正确让w为0.7 b为0.2呢?</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.生成100组x的</span></span><br><span class="line">xDist=tf.random_normal([<span class="number">100</span>,<span class="number">1</span>],mean=<span class="number">0.5</span>,stddev=<span class="number">0.5</span>,name=<span class="string">"xDist"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成目标值y的结果</span></span><br><span class="line">y_true=tf.matmul(xDist,[[<span class="number">0.7</span>]])+<span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.建立线性回归模型</span></span><br><span class="line"><span class="comment">#因为权重和偏置是需要不断训练改变的，所有需要定义成变量</span></span><br><span class="line">w=tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">1</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>),name=<span class="string">"w"</span>)</span><br><span class="line">b=tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">1</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">0.0</span>),name=<span class="string">"b"</span>)</span><br><span class="line">y=tf.matmul(xDist,w)+b</span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数和使用梯度下降优化器优化，使用最小损失优化，学习率为0.1</span></span><br><span class="line">loss=tf.reduce_mean(tf.square(y_true-y))</span><br><span class="line">train_op=tf.train.GradientDescentOptimizer(<span class="number">1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.定义全局变量全局初始化</span></span><br><span class="line">init_var=tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.开启会话开始训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#运行初始化变量</span></span><br><span class="line">    sess.run(init_var)</span><br><span class="line">    <span class="comment">#打印训练前权重和偏值,因为权重和偏置并没有被运行，所以需要使用eval方法实时获取权重和偏置</span></span><br><span class="line">    print(<span class="string">"训练前权重:%f权重:%f"</span> % (w.eval(),b.eval()))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">        sess.run(train_op)</span><br><span class="line">        print(<span class="string">"%d轮，权重为:%f,偏置为:%f"</span> % (i,w.eval(),b.eval()))</span><br></pre></td></tr></table></figure>

<h3 id="1-简述"><a href="#1-简述" class="headerlink" title="1.简述"></a>1.简述</h3><p>​        上述代码学习率是1，运行后就会发现权重和偏置变成了NAV，这就是梯度爆炸，也就是说学习率过大或者神经网络模型的某些原因就会导致梯度爆炸，但是学习率也不能过小，过小会得不到好的效果。</p>
<h3 id="2-解决方法"><a href="#2-解决方法" class="headerlink" title="2.解决方法"></a>2.解决方法</h3><ul>
<li>重新设计神经网络</li>
<li>调整学习率</li>
<li>使用梯度阶段（在训练过程中检查和限制梯度的大小）</li>
<li>使用激活函数</li>
</ul>
<h1 id="8-模型的保存和加载"><a href="#8-模型的保存和加载" class="headerlink" title="8.模型的保存和加载"></a>8.模型的保存和加载</h1><h3 id="1-代码"><a href="#1-代码" class="headerlink" title="1.代码"></a>1.代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">saver=tf.train.Saver(var_list=<span class="literal">None</span>,max_to_keep=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#在会话里面</span></span><br><span class="line">saver.restpre(sess,<span class="string">""</span>)<span class="comment">#读取模型</span></span><br><span class="line">saver.save(sess,<span class="string">""</span>)<span class="comment">#保存模型</span></span><br></pre></td></tr></table></figure>

<h6 id="var-list-自定要保持和还原的变量。他可以作为一个dict或者一个列表传进去"><a href="#var-list-自定要保持和还原的变量。他可以作为一个dict或者一个列表传进去" class="headerlink" title="var_list:自定要保持和还原的变量。他可以作为一个dict或者一个列表传进去"></a>var_list:自定要保持和还原的变量。他可以作为一个dict或者一个列表传进去</h6><h6 id="max-to-keep-制定要保留的最近检查点文件的最大数量，创建新的文件的时候会删除比较旧的文件，默认值5"><a href="#max-to-keep-制定要保留的最近检查点文件的最大数量，创建新的文件的时候会删除比较旧的文件，默认值5" class="headerlink" title="max_to_keep:制定要保留的最近检查点文件的最大数量，创建新的文件的时候会删除比较旧的文件，默认值5"></a>max_to_keep:制定要保留的最近检查点文件的最大数量，创建新的文件的时候会删除比较旧的文件，默认值5</h6><h6 id="“”-这个路径包含路径和文件名"><a href="#“”-这个路径包含路径和文件名" class="headerlink" title="“” 这个路径包含路径和文件名"></a>“” 这个路径包含路径和文件名</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#假设有一个函数关系式y=x*0.7+0.2  也就是y=x*w+b这个关系，若只知道y的结果和x的结果若干组，那么能否正确让w为0.7 b为0.2呢</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment">#1.生成100组x的</span></span><br><span class="line">xDist=tf.random_normal([<span class="number">100</span>,<span class="number">1</span>],mean=<span class="number">0.5</span>,stddev=<span class="number">0.5</span>,name=<span class="string">"xDist"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成目标值y的结果</span></span><br><span class="line">y_true=tf.matmul(xDist,[[<span class="number">0.7</span>]])+<span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.建立线性回归模型</span></span><br><span class="line"><span class="comment">#因为权重和偏置是需要不断训练改变的，所有需要定义成变量</span></span><br><span class="line">w=tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">1</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>),name=<span class="string">"w"</span>)</span><br><span class="line">b=tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">1</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">0.0</span>),name=<span class="string">"b"</span>)</span><br><span class="line">y=tf.matmul(xDist,w)+b</span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数和使用梯度下降优化器优化，使用最小损失优化，学习率为0.1</span></span><br><span class="line">loss=tf.reduce_mean(tf.square(y_true-y))</span><br><span class="line">train_op=tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#收集张量</span></span><br><span class="line">tf.summary.scalar(<span class="string">"loss"</span>,loss)</span><br><span class="line">tf.summary.histogram(<span class="string">"W"</span>,w)</span><br><span class="line"><span class="comment">#合并所有的收集到的张量</span></span><br><span class="line">margin=tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.定义全局变量全局初始化</span></span><br><span class="line">init_var=tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义保存</span></span><br><span class="line">saver=tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.开启会话开始训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#运行初始化变量</span></span><br><span class="line">    sess.run(init_var)</span><br><span class="line">    <span class="comment"># 读取模型</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(<span class="string">"./model/checkpoint"</span>):</span><br><span class="line">        saver.restore(sess, <span class="string">"./model/123"</span>)</span><br><span class="line">    <span class="comment">#写出事务文件</span></span><br><span class="line">    fileWiter=tf.summary.FileWriter(<span class="string">"./board"</span>,graph=sess.graph)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#打印训练前权重和偏值,因为权重和偏置并没有被运行，所以需要使用eval方法实时获取权重和偏置</span></span><br><span class="line">    print(<span class="string">"训练前权重:%f权重:%f"</span> % (w.eval(),b.eval()))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">        summary = sess.run(margin)</span><br><span class="line">        sess.run(train_op)</span><br><span class="line">        fileWiter.add_summary(summary,i)</span><br><span class="line">        <span class="comment"># 运行变量的合并</span></span><br><span class="line">        print(<span class="string">"%d轮，权重为:%f,偏置为:%f"</span> % (i,w.eval(),b.eval()))</span><br><span class="line">    saver.save(sess,<span class="string">"./model/123"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-保存的文件格式"><a href="#2-保存的文件格式" class="headerlink" title="2.保存的文件格式"></a>2.保存的文件格式</h3><p>​    .data-00000-of-00001和.index文件</p>
<p>​    checkpoint文件：checkpoint_dir目录下还有checkpoint文件，该文件是个文本文件，里面记录了保存的最新的checkpoint文件以及其它checkpoint文件列表。在inference时，可以通过修改这个文件，指定使用哪个model。加载restore时的文件路径名是以checkpoint文件中的“model_checkpoint_path”值决定的。</p>
<p>​    保存模型时，只会保存变量的值，placeholder里面的值不会被保存。</p>
<h1 id="9-队列机制"><a href="#9-队列机制" class="headerlink" title="9.队列机制"></a>9.队列机制</h1><h3 id="1-简述-1"><a href="#1-简述-1" class="headerlink" title="1.简述"></a>1.简述</h3><p>​    TensorFlow提供了专门的队列机制,专门用来处理文件读取的问题</p>
<h3 id="2-代码-1"><a href="#2-代码-1" class="headerlink" title="2.代码"></a>2.代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">queue=tf.FIFOQueue(capatity,dtype)<span class="comment">#定义一个队列</span></span><br></pre></td></tr></table></figure>

<h6 id="capatity-队列的容量"><a href="#capatity-队列的容量" class="headerlink" title="capatity 队列的容量"></a>capatity 队列的容量</h6><h6 id="dtype队列存储的数据类型"><a href="#dtype队列存储的数据类型" class="headerlink" title="dtype队列存储的数据类型"></a>dtype队列存储的数据类型</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">queue.dequeue()<span class="comment">#出队列并且移除</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">queue.enqueue()<span class="comment">#入队列</span></span><br><span class="line">int=queue.enqueue_many(list)<span class="comment">#入队一个列表元素</span></span><br></pre></td></tr></table></figure>

<h4 id="演示"><a href="#演示" class="headerlink" title="演示:"></a>演示:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义一个队列，不断出队列和入队列</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#定义一个队列</span></span><br><span class="line">queue=tf.FIFOQueue()</span><br><span class="line"><span class="comment">#添加队列元素</span></span><br><span class="line">int=queue.enqueue_many([[<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>],])</span><br><span class="line"><span class="comment">#定义图结构</span></span><br><span class="line">item=queue.dequeue()</span><br><span class="line">data=item+<span class="number">1</span></span><br><span class="line">en=queue.enqueue(data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始执行图结构</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#运行添加元素结构</span></span><br><span class="line">    sess.run(int)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        sess.run(en)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(queue.size().eval()):</span><br><span class="line">        print(queue.dequeue().eval())</span><br></pre></td></tr></table></figure>

<h1 id="10-文件读取"><a href="#10-文件读取" class="headerlink" title="10.文件读取"></a>10.文件读取</h1><h3 id="1-文件读取的过程"><a href="#1-文件读取的过程" class="headerlink" title="1.文件读取的过程"></a>1.文件读取的过程</h3><h5 id="1-构造文件队列"><a href="#1-构造文件队列" class="headerlink" title="1.构造文件队列"></a>1.构造文件队列</h5><h5 id="2-构造阅读器"><a href="#2-构造阅读器" class="headerlink" title="2.构造阅读器"></a>2.构造阅读器</h5><h5 id="3-对于每个样本进行解码"><a href="#3-对于每个样本进行解码" class="headerlink" title="3.对于每个样本进行解码"></a>3.对于每个样本进行解码</h5><h5 id="4-批处理文件"><a href="#4-批处理文件" class="headerlink" title="4.批处理文件"></a>4.批处理文件</h5><h3 id="2-文件读取的API介绍"><a href="#2-文件读取的API介绍" class="headerlink" title="2.文件读取的API介绍"></a>2.文件读取的API介绍</h3><h5 id="构造文件队列"><a href="#构造文件队列" class="headerlink" title="构造文件队列"></a>构造文件队列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.tarin.string_inpput_producer(string_tensor,shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h6 id="string-tensor-含有文件名的一阶张量（包含路径以及文件名的列表）"><a href="#string-tensor-含有文件名的一阶张量（包含路径以及文件名的列表）" class="headerlink" title="string_tensor 含有文件名的一阶张量（包含路径以及文件名的列表）"></a>string_tensor 含有文件名的一阶张量（包含路径以及文件名的列表）</h6><h6 id="shuffle-是否乱序-默认乱序"><a href="#shuffle-是否乱序-默认乱序" class="headerlink" title="shuffle 是否乱序 默认乱序"></a>shuffle 是否乱序 默认乱序</h6><h6 id="num-epochs-过几遍数据，默认无限过数据"><a href="#num-epochs-过几遍数据，默认无限过数据" class="headerlink" title="num_epochs 过几遍数据，默认无限过数据"></a>num_epochs 过几遍数据，默认无限过数据</h6><h5 id="构造文件阅读器"><a href="#构造文件阅读器" class="headerlink" title="构造文件阅读器"></a>构造文件阅读器</h5><ul>
<li><p>所有阅读器解码出来形状都是不固定的，注意后边进行形状固定</p>
</li>
<li><p>要根据文件类型选择对应的文件阅读区</p>
</li>
<li><p>每个read方法返回key，和value参数，其中key表示文件名称，value表示每个样本</p>
<p>文本文件和CSV文件:</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class reader=tf.TextLineReader()#构造文件阅读器</span><br><span class="line">	reader.read(file_queue)<span class="comment">#读取一个样本</span></span><br></pre></td></tr></table></figure>

<h6 id="文本文件阅读器，默认按行读取，（因为对于csv文件和文本文件来说，一行就是一个样本）"><a href="#文本文件阅读器，默认按行读取，（因为对于csv文件和文本文件来说，一行就是一个样本）" class="headerlink" title="文本文件阅读器，默认按行读取，（因为对于csv文件和文本文件来说，一行就是一个样本）"></a>文本文件阅读器，默认按行读取，（因为对于csv文件和文本文件来说，一行就是一个样本）</h6><h6 id="return-返回阅读器实例"><a href="#return-返回阅读器实例" class="headerlink" title="return 返回阅读器实例"></a>return 返回阅读器实例</h6><h5 id="read-file-queue"><a href="#read-file-queue" class="headerlink" title="read(file_queue)"></a>read(file_queue)</h5><h6 id="file-queue-从队列里面读取内容"><a href="#file-queue-从队列里面读取内容" class="headerlink" title="file_queue 从队列里面读取内容"></a>file_queue 从队列里面读取内容</h6><h5 id="二进制文件阅读器"><a href="#二进制文件阅读器" class="headerlink" title="二进制文件阅读器"></a>二进制文件阅读器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class reader=tf.FixedLengthRecordReader(record_bytes)#构造文件阅读器</span><br><span class="line">	reader.read(file_queue)<span class="comment">#读取一个样本文件</span></span><br></pre></td></tr></table></figure>

<h6 id="读取每个样本是按固定数量的字节读取的二进制文件"><a href="#读取每个样本是按固定数量的字节读取的二进制文件" class="headerlink" title="读取每个样本是按固定数量的字节读取的二进制文件"></a>读取每个样本是按固定数量的字节读取的二进制文件</h6><h6 id="record-bytes-整形，指定每次读取的字节数"><a href="#record-bytes-整形，指定每次读取的字节数" class="headerlink" title="record_bytes:整形，指定每次读取的字节数"></a>record_bytes:整形，指定每次读取的字节数</h6><h6 id="return-返回阅读器实例-1"><a href="#return-返回阅读器实例-1" class="headerlink" title="return 返回阅读器实例"></a>return 返回阅读器实例</h6><h5 id="图片文件阅读器"><a href="#图片文件阅读器" class="headerlink" title="图片文件阅读器"></a>图片文件阅读器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class reader=tf.WholeFileReader()#构造图片文件阅读器</span><br><span class="line">	reader.read(file_queue)<span class="comment">#读取一个图片样本</span></span><br></pre></td></tr></table></figure>

<h5 id="每个样本进行解码"><a href="#每个样本进行解码" class="headerlink" title="每个样本进行解码"></a>每个样本进行解码</h5><h5 id="CSV文件解码"><a href="#CSV文件解码" class="headerlink" title="CSV文件解码"></a>CSV文件解码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.decode_csv(value,record_defaults=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h6 id="value表示待解码的内容"><a href="#value表示待解码的内容" class="headerlink" title="value表示待解码的内容"></a>value表示待解码的内容</h6><h6 id="record-defaults-表示每个样本如何解码，并且缺失的时候的默认值-1-表示一个样本按整形解码，缺失的时候为1，-“None”-1-0-表示样本有两个第一个按string类型解码，缺失的时候是None-第二个按float类型解码，丢失按1-0处理"><a href="#record-defaults-表示每个样本如何解码，并且缺失的时候的默认值-1-表示一个样本按整形解码，缺失的时候为1，-“None”-1-0-表示样本有两个第一个按string类型解码，缺失的时候是None-第二个按float类型解码，丢失按1-0处理" class="headerlink" title="record_defaults 表示每个样本如何解码，并且缺失的时候的默认值 [[1]]表示一个样本按整形解码，缺失的时候为1，[[“None”],[1.0]] 表示样本有两个第一个按string类型解码，缺失的时候是None,第二个按float类型解码，丢失按1.0处理"></a>record_defaults 表示每个样本如何解码，并且缺失的时候的默认值 [[1]]表示一个样本按整形解码，缺失的时候为1，[[“None”],[1.0]] 表示样本有两个第一个按string类型解码，缺失的时候是None,第二个按float类型解码，丢失按1.0处理</h6><h5 id="图片文件解码"><a href="#图片文件解码" class="headerlink" title="图片文件解码"></a>图片文件解码</h5> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.image.decode_jpeg(contents)</span><br><span class="line"><span class="comment">#将JPEG编码的图像解码为uint8的张亮</span></span><br><span class="line"><span class="comment">#return:uint8张量3-D形状[height,width,hannels]</span></span><br><span class="line">tf.image.decode_png(contents)</span><br><span class="line"><span class="comment">#将PNG图片的图像解码为uint8或者uint16的张量</span></span><br><span class="line"><span class="comment">#return:张量类型，3-D形状[height,width,hannels]</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>图片文件缩放</p>
</li>
<li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.image.resize_images(images,size)</span><br><span class="line">images:<span class="number">4</span>-D形状[batch,height,width,channels]或者<span class="number">3</span>-D的形状的张量[height,width,channels]</span><br><span class="line">size图片的新尺寸，new_height,new_width</span><br></pre></td></tr></table></figure>



</li>
</ul>
<h5 id="管道批处理文件"><a href="#管道批处理文件" class="headerlink" title="管道批处理文件"></a>管道批处理文件</h5> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#批处理 batch_size表示每批的数量，num_threads进行的线程数量，capacity批处理管道的容量</span></span><br><span class="line">    ones,twos=tf.train.batch([one,two],batch_size=<span class="number">6</span>,num_threads=<span class="number">1</span>,capacity=<span class="number">9</span>)</span><br></pre></td></tr></table></figure>

<h3 id="3-文件读取的简单演示"><a href="#3-文件读取的简单演示" class="headerlink" title="3.文件读取的简单演示"></a>3.文件读取的简单演示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.构造文件队列（路径加文件名）</span></span><br><span class="line"><span class="comment">#2.构造文件阅读器</span></span><br><span class="line"><span class="comment">#3.按每个样本解码（转化为张量 ）</span></span><br><span class="line"><span class="comment">#4.构造批处理</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###在同目录下有个data文件夹，里面的csv文件里面都有两行且都是string类型</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">#找到对应的文件目录获取文件路径加列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fileRead</span><span class="params">(path)</span>:</span></span><br><span class="line">    fileName=os.listdir(path)</span><br><span class="line">    filePath=[os.path.join(path,file) <span class="keyword">for</span> file <span class="keyword">in</span> fileName]</span><br><span class="line">    <span class="keyword">return</span> filePath</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">csvRead</span><span class="params">(fileList)</span>:</span></span><br><span class="line">    <span class="comment">#构造文件队列</span></span><br><span class="line">    fileQueue=tf.train.string_input_producer(fileList)</span><br><span class="line">    <span class="comment">#构造阅读器</span></span><br><span class="line">    reader=tf.TextLineReader()</span><br><span class="line">    key,value=reader.read(fileQueue)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#对文件进行解码</span></span><br><span class="line">    <span class="comment">#设定每行的类型以及每行的默认值</span></span><br><span class="line">    cord=[[<span class="string">"None"</span>],[<span class="string">"None"</span>]]</span><br><span class="line">    one,two=tf.decode_csv(value,record_defaults=cord)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#批处理 batch_size表示每批的数量，num_threads进行的线程数量，capacity批处理管道的容量</span></span><br><span class="line">    ones,twos=tf.train.batch([one,two],batch_size=<span class="number">6</span>,num_threads=<span class="number">1</span>,capacity=<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ones,twos</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    fileList=fileRead(<span class="string">"./data"</span>)</span><br><span class="line">    one,two=csvRead(fileList)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment">#定义一个线程协调器</span></span><br><span class="line">        coord=tf.train.Coordinator()</span><br><span class="line">        <span class="comment">#开启一个线程</span></span><br><span class="line">        thread=tf.train.start_queue_runners(sess,coord=coord)</span><br><span class="line">        print(sess.run([one,two]))</span><br><span class="line">        <span class="comment">#回收子线程</span></span><br><span class="line">        coord.request_stop()</span><br><span class="line">        coord.join(thread)</span><br></pre></td></tr></table></figure>

<h3 id="图片文件的读取简单演示"><a href="#图片文件的读取简单演示" class="headerlink" title="图片文件的读取简单演示"></a>图片文件的读取简单演示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读取当前文件夹下面的data目录下面所有的jpg格式图片的信息</span></span><br><span class="line"><span class="comment">#步骤</span></span><br><span class="line"><span class="comment"># 1.获取path文件下面的所有图片的全路径列表</span></span><br><span class="line"><span class="comment"># 2.构造文件队列</span></span><br><span class="line"><span class="comment"># 3.构造图片阅读器</span></span><br><span class="line"><span class="comment"># 4.图片解码</span></span><br><span class="line"><span class="comment"># 5.图片缩放</span></span><br><span class="line"><span class="comment"># 6.图片调整维度</span></span><br><span class="line"><span class="comment"># 7.批处理</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取path目录下所有的图片信息</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPath</span><span class="params">(path)</span>:</span></span><br><span class="line">    fileNames=os.listdir(path)</span><br><span class="line">    filePath=[os.path.join(path,fileName) <span class="keyword">for</span> fileName <span class="keyword">in</span> fileNames]</span><br><span class="line">    <span class="keyword">return</span> filePath</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取图片并进行批处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readImg</span><span class="params">(filePathList)</span>:</span></span><br><span class="line">    <span class="comment">#1.构造文件队列</span></span><br><span class="line">    fileQueue=tf.train.string_input_producer(filePathList)</span><br><span class="line">    <span class="comment">#1.构造图片阅读器</span></span><br><span class="line">    reader=tf.WholeFileReader()</span><br><span class="line">    key,value=reader.read(fileQueue)</span><br><span class="line">    print(<span class="string">"构造完阅读器"</span>,value)</span><br><span class="line">    <span class="comment">#3.图片解码</span></span><br><span class="line">    image=tf.image.decode_jpeg(value)</span><br><span class="line">    print(<span class="string">"解码"</span>,image)</span><br><span class="line">    <span class="comment">#4.图片缩放</span></span><br><span class="line">    reImage=tf.image.resize_images(image,[<span class="number">200</span>,<span class="number">200</span>])</span><br><span class="line">    print(<span class="string">"图片放缩后"</span>,reImage)</span><br><span class="line">    <span class="comment">#5.图片定型 静态调整</span></span><br><span class="line">    reImage.set_shape([<span class="number">200</span>,<span class="number">200</span>,<span class="number">3</span>])</span><br><span class="line">    print(<span class="string">"图片静态调整后"</span>,reImage)</span><br><span class="line">    <span class="comment">#文件批处理</span></span><br><span class="line">    jpg=tf.train.batch([reImage],batch_size=<span class="number">5</span>,num_threads=<span class="number">2</span>,capacity=<span class="number">5</span>)</span><br><span class="line">    <span class="keyword">return</span> reImage</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    filePathList=getPath(<span class="string">"./data"</span>)</span><br><span class="line">    jpg=readImg(filePathList)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#开启会话</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment">#定义一个线程协调器</span></span><br><span class="line">        coord=tf.train.Coordinator()</span><br><span class="line">        <span class="comment">#定义一个线程</span></span><br><span class="line">        thread=tf.train.start_queue_runners(sess,coord=coord)</span><br><span class="line">        print(sess.run(jpg))</span><br><span class="line"></span><br><span class="line">        <span class="comment">#回收子线程</span></span><br><span class="line">        coord.request_stop()</span><br><span class="line">        coord.join(thread)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>注意打印结果:</p>
<p>构造完阅读器 Tensor(“ReaderReadV2:1”, shape=(), dtype=string)<br>解码 Tensor(“DecodeJpeg:0”, shape=(?, ?, ?), dtype=uint8)<br>图片放缩后 Tensor(“Squeeze:0”, shape=(200, 200, ?), dtype=float32)<br>图片静态调整后 Tensor(“Squeeze:0”, shape=(200, 200, 3), dtype=float32)</p>
</li>
</ul>
<h1 id="11-交叉熵损失计算和softMax计算"><a href="#11-交叉熵损失计算和softMax计算" class="headerlink" title="11.交叉熵损失计算和softMax计算"></a>11.交叉熵损失计算和softMax计算</h1><h3 id="1-softMax计算"><a href="#1-softMax计算" class="headerlink" title="1.softMax计算"></a>1.softMax计算</h3><p>​    假设我们有一个数组，V，Vi表示V中的第i个元素，那么这个元素的Softmax值就是</p>
<p>​    <img src="C:%5CUsers%5Chp%5CPictures%5Cequation.svg" alt></p>
<p>可以计算n个结果之间发生的概率</p>
<h3 id="2-交叉熵损失"><a href="#2-交叉熵损失" class="headerlink" title="2.交叉熵损失"></a>2.交叉熵损失</h3><p>​    可以和onehost编码与softMax计算损失</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#SoftMax和交叉熵损失计算</span></span><br><span class="line">tf.nn.softmax_cross_entropy_with_logits(babels=<span class="literal">None</span>,logits=<span class="literal">None</span>,name)</span><br><span class="line"><span class="comment">#计算logits和labels之间的交叉熵损失</span></span><br><span class="line"><span class="comment">#labels:真实值</span></span><br><span class="line"><span class="comment">#logits:预测值</span></span><br><span class="line"><span class="comment">#return:返回所有样本的损失值列表</span></span><br></pre></td></tr></table></figure>

<h3 id="3-演示"><a href="#3-演示" class="headerlink" title="3.演示"></a>3.演示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(input,</span></span></span><br><span class="line"><span class="function"><span class="params">           axis=None,</span></span></span><br><span class="line"><span class="function"><span class="params">           name=None,</span></span></span><br><span class="line"><span class="function"><span class="params">           dimension=None,</span></span></span><br><span class="line"><span class="function"><span class="params">           output_type=dtypes.int64)</span></span></span><br><span class="line"><span class="function"><span class="title">numpy</span>.<span class="title">argmax</span><span class="params">(a, axis=None, out=None)</span> </span></span><br><span class="line"><span class="function">返回沿轴<span class="title">axis</span>最大值的索引。</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">Parameters</span>:</span> </span><br><span class="line">input: array_like，数组</span><br><span class="line">axis : int, 可选，默认情况下，索引的是平铺的数组，否则沿指定的轴。 </span><br><span class="line">out : array, 可选 如果提供，结果以合适的形状和类型被插入到此数组中。 </span><br><span class="line"></span><br><span class="line">Returns: </span><br><span class="line">index_array : ndarray of ints </span><br><span class="line">索引数组。它具有与a.shape相同的形状，其中axis被移除。</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#简单的神经网络识别手写数字</span></span><br><span class="line"><span class="comment">#1.定义占位符</span></span><br><span class="line"><span class="comment">#2.搭建神经网络</span></span><br><span class="line"><span class="comment">#3.计算损失</span></span><br><span class="line"><span class="comment">#4.反向传播优化损失</span></span><br><span class="line"><span class="comment">#5.计算准确率</span></span><br><span class="line"><span class="comment">#6.开启会话训练</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imgNn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#1.定义占位符</span></span><br><span class="line">    xDist=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line">    yTrue=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line">    <span class="comment">#2.初始化变量搭建神经网络</span></span><br><span class="line">    w=tf.Variable(tf.random_normal([<span class="number">784</span>,<span class="number">10</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>))</span><br><span class="line">    b=tf.Variable(tf.random_normal([<span class="number">10</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>))</span><br><span class="line">    y=tf.matmul(xDist,w)+b</span><br><span class="line">    <span class="comment">#3.计算平均交叉熵损失率</span></span><br><span class="line">    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=yTrue,logits=y))</span><br><span class="line">    <span class="comment">#4.反向传播最小优化学习率0.1</span></span><br><span class="line">    trainOp=tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line">    <span class="comment">#5.计算准确率 arg_max会反正正确结果的下标 1表示按同行比较 0表示同列</span></span><br><span class="line">    equal_list=tf.equal(tf.arg_max(yTrue,<span class="number">1</span>),tf.arg_max(y,<span class="number">1</span>))</span><br><span class="line">    acuracy=tf.reduce_mean(tf.cast(equal_list,tf.float32))</span><br><span class="line">    initOp=tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#6.开启会话开始训练</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(initOp)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3000</span>):</span><br><span class="line">            <span class="comment">#取出特征值和目标值</span></span><br><span class="line">            minstX,minstY=mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">            sess.run(trainOp,feed_dict=&#123;xDist:minstX,yTrue:minstY&#125;)</span><br><span class="line">            print(<span class="string">"%d步准确率%f"</span> % (i,sess.run(acuracy,feed_dict=&#123;xDist:minstX,yTrue:minstY&#125;)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">"__main__"</span>:</span><br><span class="line">    imgNn()</span><br></pre></td></tr></table></figure>

<h1 id="12-卷积神经网络"><a href="#12-卷积神经网络" class="headerlink" title="12.卷积神经网络"></a>12.卷积神经网络</h1><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h3><p>​    卷积神经网络（Convolutional Neural Networks / CNNs / ConvNets）与普通神经网络非常相似，它们都由具有可学习的权重和偏置常量(biases)的神经元组成。每个神经元都接收一些输入，并做一些点积计算，输出是每个分类的分数，普通神经网络里的一些计算技巧到这里依旧适用。</p>
<p>具有三维体积的神经元(3D volumes of neurons) </p>
<p>​    卷积神经网络利用输入是图片的特点，把神经元设计成三个维度 ： width, height, depth(注意这个depth不是神经网络的深度，而是用来描述神经元的) 。比如输入的图片大小是 32 × 32 × 3 (rgb)，那么输入神经元就也具有 32×32×3 的维度。</p>
<h3 id="2-卷积神经网络分层"><a href="#2-卷积神经网络分层" class="headerlink" title="2.卷积神经网络分层"></a>2.卷积神经网络分层</h3><h4 id="卷积层—-gt-激活层—-gt-池化层—-gt-全连接层"><a href="#卷积层—-gt-激活层—-gt-池化层—-gt-全连接层" class="headerlink" title="卷积层—&gt;激活层—&gt;池化层—&gt;全连接层"></a>卷积层—&gt;激活层—&gt;池化层—&gt;全连接层</h4><pre><code>##### 卷积层：</code></pre><p>​    卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。</p>
<h5 id="激活层："><a href="#激活层：" class="headerlink" title="激活层："></a>激活层：</h5><p>​    通过Relu激活函数可以增加网络的非线性分割能力</p>
<h5 id="池化层："><a href="#池化层：" class="headerlink" title="池化层："></a>池化层：</h5><p>​    用来减少数据量</p>
<h3 id="3-填充算法"><a href="#3-填充算法" class="headerlink" title="3.填充算法"></a>3.填充算法</h3><h4 id="SAME："><a href="#SAME：" class="headerlink" title="SAME："></a>SAME：</h4><p>​    当filter过滤到边缘的时候自动填充0</p>
<pre><code>- 越过边缘取样，取样的面积和输入的图像像素长度和宽度相同</code></pre><h4 id="VALID："><a href="#VALID：" class="headerlink" title="VALID："></a>VALID：</h4><ul>
<li><p>当filter过滤到边缘的时候跳过，所有会丢失部分特征，取样的面积和输入的图像像素长度和宽度会略小</p>
<p>计算公式为：输入体积大小h1 * w1 * d1 ，filter数量为k,filter大小为f,步长为s,填充大小为p</p>
<p>那么</p>
<p>h2=(h1-f+2p)/s+1</p>
<p>w2=(w1-f+2p)/s+1</p>
<p>d2=k    </p>
</li>
</ul>
<h3 id="4-API介绍"><a href="#4-API介绍" class="headerlink" title="4.API介绍"></a>4.API介绍</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#卷积层</span></span><br><span class="line">tf.nn.conv2d(input,filter,strides=,padding=,name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h6 id="input-输入的4-D张量，具有-batch-height-width-channel-类型为float32，或者float64-每批数-图片长度，图片宽度，图片通道数"><a href="#input-输入的4-D张量，具有-batch-height-width-channel-类型为float32，或者float64-每批数-图片长度，图片宽度，图片通道数" class="headerlink" title="input: 输入的4-D张量，具有[batch,height,width,channel],类型为float32，或者float64([每批数,图片长度，图片宽度，图片通道数])"></a>input: 输入的4-D张量，具有[batch,height,width,channel],类型为float32，或者float64([每批数,图片长度，图片宽度，图片通道数])</h6><h6 id="filter-指定过滤器4-D随机初始化的张量：-类型为float32或者float64-过滤器长度，过滤器宽度，输入的通道数，输出的通道数-，"><a href="#filter-指定过滤器4-D随机初始化的张量：-类型为float32或者float64-过滤器长度，过滤器宽度，输入的通道数，输出的通道数-，" class="headerlink" title="filter:指定过滤器4-D随机初始化的张量：,类型为float32或者float64([过滤器长度，过滤器宽度，输入的通道数，输出的通道数])，"></a>filter:指定过滤器4-D随机初始化的张量：,类型为float32或者float64([过滤器长度，过滤器宽度，输入的通道数，输出的通道数])，</h6><h6 id="——-注意需要传入4-D张量，而不是简单的填入维度"><a href="#——-注意需要传入4-D张量，而不是简单的填入维度" class="headerlink" title="——- 注意需要传入4-D张量，而不是简单的填入维度"></a>——- 注意需要传入4-D张量，而不是简单的填入维度</h6><h6 id="strides-strides-1-stride-stride-1-步长（-1，长上步长，宽上步长，1-）"><a href="#strides-strides-1-stride-stride-1-步长（-1，长上步长，宽上步长，1-）" class="headerlink" title="strides:strides=[1,stride,stride,1],步长（[1，长上步长，宽上步长，1]）"></a>strides:strides=[1,stride,stride,1],步长（[1，长上步长，宽上步长，1]）</h6><h6 id="padding-”SAME”，“VALID”，使用的填充算法类型"><a href="#padding-”SAME”，“VALID”，使用的填充算法类型" class="headerlink" title="padding=:”SAME”，“VALID”，使用的填充算法类型."></a>padding=:”SAME”，“VALID”，使用的填充算法类型.</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#relu激活函数</span></span><br><span class="line">tf.nn.relu(feacutes,name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h6 id="features-卷积后加上偏置的结果"><a href="#features-卷积后加上偏置的结果" class="headerlink" title="features:卷积后加上偏置的结果"></a>features:卷积后加上偏置的结果</h6><h6 id="return-结果"><a href="#return-结果" class="headerlink" title="return :结果"></a>return :结果</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#池化</span></span><br><span class="line">tf.nn.max_pool(value,ksize=,strides=,padding=,name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h6 id="value-4-D-tensor形状-batch-height-width-channels-，也就是激活函数处理后的结果"><a href="#value-4-D-tensor形状-batch-height-width-channels-，也就是激活函数处理后的结果" class="headerlink" title="value:4-D tensor形状[batch,height,width,channels]，也就是激活函数处理后的结果"></a>value:4-D tensor形状[batch,height,width,channels]，也就是激活函数处理后的结果</h6><h6 id="ksize池化窗口大小，-1-ksize-ksize-1"><a href="#ksize池化窗口大小，-1-ksize-ksize-1" class="headerlink" title="ksize池化窗口大小，[1,ksize,ksize,1]"></a>ksize池化窗口大小，[1,ksize,ksize,1]</h6><h6 id="步长大小-1-strides-strides-1"><a href="#步长大小-1-strides-strides-1" class="headerlink" title="步长大小,[1,strides,strides,1]"></a>步长大小,[1,strides,strides,1]</h6><h6 id="padding-”SAME”-”VALID”填充算法"><a href="#padding-”SAME”-”VALID”填充算法" class="headerlink" title="padding :”SAME”,”VALID”填充算法"></a>padding :”SAME”,”VALID”填充算法</h6><h3 id="5-演示"><a href="#5-演示" class="headerlink" title="5.演示"></a>5.演示</h3><h5 id="两层卷积神经网络识别手写数字"><a href="#两层卷积神经网络识别手写数字" class="headerlink" title="两层卷积神经网络识别手写数字"></a>两层卷积神经网络识别手写数字</h5><p>​    1.输入数据形状[None,784]</p>
<pre><code>##### 第一层卷积神经网络</code></pre><h6 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h6><p>​    2.因为卷积API的input即输入为4-D张量，所以动态改变形状改为[None,28,28,1],使用32个filter的5*5大小，步长为1，SAME填充算法的过滤器且卷积后输出的形状为[None,28,28,32]，有多少个filter就有多少个偏置所以为32，权重就是每个filter的值.</p>
<pre><code>######     激活层</code></pre><p>​    3.不改变数据大小，所以输出还是[None,28,28,32]</p>
<pre><code>######     池化</code></pre><p>​    4.大小为2 *2，步长为2，填充算法为“SAME”(这里SAME后的大小比原来小)将[None,28,28,32]的图像池化为[None,14,14,32]</p>
<h5 id="第一层卷积神经网络"><a href="#第一层卷积神经网络" class="headerlink" title="第一层卷积神经网络"></a>第一层卷积神经网络</h5><h6 id="卷积层-1"><a href="#卷积层-1" class="headerlink" title="卷积层"></a>卷积层</h6><p>​    2.因为卷积API的input即输入为4-D张量，输入为[None,14,14,32],使用64个filter的5*5大小，步长为1，SAME填充算法的过滤器且卷积后输出的形状为[None,14,14,64]，有多少个filter就有多少个偏置所以为64,权重就是每个filter的值.</p>
<h6 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h6><p>​    3.不改变数据大小，所以输出还是[None,14,14,64]</p>
<h6 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h6><p>​    4.大小为2*2，步长为2，填充算法为“SAME”(这里SAME后的大小比原来小)将[None,14,14,64]的图像池化为[None,7,7,64]</p>
<h5 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h5><p>​    因为池化后数据为[None,7,7,64],而且矩阵乘法只能处理二维数据，所以动态调整形状为[None,7 * 7 *64],又因为输出是10种结果，所以全连接层为[7 * 7 *64,10],所以权重[7 * 7 *64,10]偏置有10个</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##### 两层卷积神经网络识别手写数字</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getVarRandom_normal</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.random_normal(shape=shape,mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#搭建神经网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conV</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#1.定义占位符</span></span><br><span class="line">    xDist=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line">    yTrue=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#2.搭建第一层卷积网络 filter 有32个 大小为5*5，步长为1，</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"con1"</span>):</span><br><span class="line">        xshape=tf.reshape(xDist,[<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line">        filterW=getVarRandom_normal([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>])</span><br><span class="line">        b=getVarRandom_normal([<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment">#卷积层</span></span><br><span class="line">        con1=tf.nn.conv2d(xshape,filterW,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)+b</span><br><span class="line"></span><br><span class="line">        <span class="comment">#激活层</span></span><br><span class="line">        relu=tf.nn.relu(con1)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#池化层 大小为2*2 步长为2 [None,28,28,32]----&gt;[None,14,14,32]</span></span><br><span class="line">        pool=tf.nn.max_pool(relu,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#3.搭建第二层卷积网络 filter有64个，大小为5*5，步长为1</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"con2"</span>):</span><br><span class="line">        filterW2=getVarRandom_normal([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>])</span><br><span class="line">        b1=getVarRandom_normal([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment">#卷积层</span></span><br><span class="line">        con2=tf.nn.conv2d(pool,filterW2,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)+b1</span><br><span class="line"></span><br><span class="line">        <span class="comment">#激活层</span></span><br><span class="line">        relu2=tf.nn.relu(con2)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#池化层</span></span><br><span class="line">        pool2=tf.nn.max_pool(relu2,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#全连接层</span></span><br><span class="line">    w3=getVarRandom_normal([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>,<span class="number">10</span>])</span><br><span class="line">    b3=getVarRandom_normal([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    shape = tf.reshape(pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">    grop=tf.matmul(shape,w3)+b3</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> xDist,yTrue,grop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    xDist,yTrue,opInt=conV()</span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算平均交叉熵损失</span></span><br><span class="line">    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=yTrue,logits=opInt))</span><br><span class="line">    <span class="comment">#反向传播</span></span><br><span class="line">    trainOp=tf.train.GradientDescentOptimizer(<span class="number">0.00001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算准确率</span></span><br><span class="line">    equal_list = tf.equal(tf.argmax(yTrue, <span class="number">1</span>), tf.argmax(opInt, <span class="number">1</span>))</span><br><span class="line">    acuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))</span><br><span class="line">    initOp = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6.开启会话开始训练</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(initOp)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3000000</span>):</span><br><span class="line">            <span class="comment"># 取出特征值和目标值</span></span><br><span class="line">            minstX, minstY = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">            sess.run(trainOp, feed_dict=&#123;xDist: minstX, yTrue: minstY&#125;)</span><br><span class="line">            print(<span class="string">"%d步准确率%f"</span> % (i, sess.run(acuracy, feed_dict=&#123;xDist: minstX, yTrue: minstY&#125;)))</span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    yang295513
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://yoursite.com/2018/05/27/1.tensorflow基本介绍/" title="Tensorflow入门教程">http://yoursite.com/2018/05/27/1.tensorflow基本介绍/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/语言/" rel="tag"># 语言</a>
          
            <a href="/tags/编程/" rel="tag"># 编程</a>
          
            <a href="/tags/日常学习/" rel="tag"># 日常学习</a>
          
            <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
          
            <a href="/tags/Python/" rel="tag"># Python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/27/Git使用入门/" rel="next" title="git使用入门">
                <i class="fa fa-chevron-left"></i> git使用入门
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/27/CC++语言位运算基本用法和骚操作/" rel="prev" title="C/C++语言位运算基本用法和骚操作">
                C/C++语言位运算基本用法和骚操作 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yang295513</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-tensorflow基本介绍"><span class="nav-number">1.</span> <span class="nav-text">1.tensorflow基本介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-TensorFlow-简介"><span class="nav-number">1.0.1.</span> <span class="nav-text">1.TensorFlow 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-TensorFlow基本术语"><span class="nav-number">1.0.2.</span> <span class="nav-text">2.TensorFlow基本术语</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#张量（tensor）："><span class="nav-number">1.0.2.1.</span> <span class="nav-text">张量（tensor）：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图（graph）"><span class="nav-number">1.0.2.2.</span> <span class="nav-text">图（graph）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#节点（op）"><span class="nav-number">1.0.2.3.</span> <span class="nav-text">节点（op）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#会话-Session"><span class="nav-number">1.0.2.4.</span> <span class="nav-text">会话(Session)</span></a></li></ol></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#2-张量（tensor）的使用以及注意事项"><span class="nav-number">2.</span> <span class="nav-text">2.张量（tensor）的使用以及注意事项</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-张量的基本概念"><span class="nav-number">2.0.0.1.</span> <span class="nav-text">1.张量的基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-张量的数据类型"><span class="nav-number">2.0.0.2.</span> <span class="nav-text">2.张量的数据类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-张量的代码"><span class="nav-number">2.0.0.3.</span> <span class="nav-text">3.张量的代码</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#其中Const表示进行的op操作名字"><span class="nav-number">2.0.0.3.1.</span> <span class="nav-text">其中Const表示进行的op操作名字</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#shape表示张量的维度-表示标量，（1）表示向量，（2，3）表示2行3列的张量"><span class="nav-number">2.0.0.3.2.</span> <span class="nav-text">shape表示张量的维度,()表示标量，（1）表示向量，（2，3）表示2行3列的张量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dtype表示张量的数据类型"><span class="nav-number">2.0.0.3.3.</span> <span class="nav-text">dtype表示张量的数据类型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-生成张量"><span class="nav-number">2.0.1.</span> <span class="nav-text">4.生成张量</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#创建一个常数张量"><span class="nav-number">2.0.1.0.1.</span> <span class="nav-text">创建一个常数张量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#固定值初始化"><span class="nav-number">2.0.1.0.2.</span> <span class="nav-text">固定值初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#tf-zeros-n-m-tf-dtype"><span class="nav-number">2.0.1.0.2.1.</span> <span class="nav-text">tf.zeros([n,m],tf.dtype)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#tf-ones-n-m-tf-dtype"><span class="nav-number">2.0.1.0.2.2.</span> <span class="nav-text">tf.ones([n,m],tf.dtype)</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#随机值初始化"><span class="nav-number">2.0.1.0.3.</span> <span class="nav-text">随机值初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#tf-random-normal-n-m-mean-2-0-stddev-4-seed-12"><span class="nav-number">2.0.1.0.3.1.</span> <span class="nav-text">tf.random_normal([n,m],mean=2.0,stddev=4,seed=12)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-占位符"><span class="nav-number">2.0.2.</span> <span class="nav-text">5.占位符</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-张量的维度调整"><span class="nav-number">2.0.3.</span> <span class="nav-text">6.张量的维度调整</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#区别"><span class="nav-number">2.0.3.1.</span> <span class="nav-text">区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#静态调整"><span class="nav-number">2.0.3.2.</span> <span class="nav-text">静态调整</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#语法-张量名字-set-shape-n-m-调整张量为n行m列"><span class="nav-number">2.0.3.2.1.</span> <span class="nav-text">语法: 张量名字.set_shape([n,m]) 调整张量为n行m列</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#动态调整"><span class="nav-number">2.0.3.3.</span> <span class="nav-text">动态调整</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#语法-tf-reshape-tensor-shape-name-None"><span class="nav-number">2.0.3.3.1.</span> <span class="nav-text">语法: tf.reshape(tensor,shape,name=None)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-改变张量的类型"><span class="nav-number">2.0.4.</span> <span class="nav-text">7.改变张量的类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-张量的切片和扩展"><span class="nav-number">2.0.5.</span> <span class="nav-text">8.张量的切片和扩展</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-会话（Session）"><span class="nav-number">3.</span> <span class="nav-text">3.会话（Session）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本写法"><span class="nav-number">3.0.1.</span> <span class="nav-text">基本写法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-run方法"><span class="nav-number">3.0.2.</span> <span class="nav-text">1.run方法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#fetches-运行ops和计算tensor"><span class="nav-number">3.0.2.0.0.1.</span> <span class="nav-text">fetches 运行ops和计算tensor</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#feed-dict-可选项（字典类型），提取使用占位符之后给图提供数据"><span class="nav-number">3.0.2.0.0.2.</span> <span class="nav-text">feed_dict 可选项（字典类型），提取使用占位符之后给图提供数据</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-变量（Variable）"><span class="nav-number">4.</span> <span class="nav-text">4.变量（Variable）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-变量的定义"><span class="nav-number">4.0.0.0.1.</span> <span class="nav-text">1.变量的定义</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#initial-value-值-可以用正态分布或者固定生成张量的值"><span class="nav-number">4.0.0.0.1.1.</span> <span class="nav-text">initial_value 值 可以用正态分布或者固定生成张量的值</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-TensorBoard-的使用和用法"><span class="nav-number">5.</span> <span class="nav-text">4.TensorBoard 的使用和用法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-TensorBoard的简介"><span class="nav-number">5.0.0.0.1.</span> <span class="nav-text">1.TensorBoard的简介</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-TensorBoard代码"><span class="nav-number">5.0.0.0.2.</span> <span class="nav-text">2.TensorBoard代码</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#logdir事务文件的绝对路径"><span class="nav-number">5.0.0.0.2.1.</span> <span class="nav-text">logdir事务文件的绝对路径</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#graph写出事务文件的图"><span class="nav-number">5.0.0.0.2.2.</span> <span class="nav-text">graph写出事务文件的图</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#name-表示TensorBoard里面显示的名称"><span class="nav-number">5.0.0.0.2.3.</span> <span class="nav-text">name 表示TensorBoard里面显示的名称</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#tensor表示要收集的张量"><span class="nav-number">5.0.0.0.2.4.</span> <span class="nav-text">tensor表示要收集的张量</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-TensorBoard的演示"><span class="nav-number">5.0.0.0.3.</span> <span class="nav-text">3.TensorBoard的演示</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-损失函数"><span class="nav-number">6.</span> <span class="nav-text">5.损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-均方误差"><span class="nav-number">6.0.1.</span> <span class="nav-text">1.均方误差</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-梯度下降算法"><span class="nav-number">7.</span> <span class="nav-text">6.梯度下降算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-基本思想"><span class="nav-number">7.0.1.</span> <span class="nav-text">1.基本思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-代码"><span class="nav-number">7.0.2.</span> <span class="nav-text">2.代码</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#learning-rate-学习率-通常填写0-0到1-0之间的浮点数"><span class="nav-number">7.0.2.0.0.1.</span> <span class="nav-text">learning_rate 学习率 通常填写0.0到1.0之间的浮点数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-简单的线性回归案例"><span class="nav-number">8.</span> <span class="nav-text">6.简单的线性回归案例</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-梯度爆炸"><span class="nav-number">9.</span> <span class="nav-text">7.梯度爆炸</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-简述"><span class="nav-number">9.0.1.</span> <span class="nav-text">1.简述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-解决方法"><span class="nav-number">9.0.2.</span> <span class="nav-text">2.解决方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8-模型的保存和加载"><span class="nav-number">10.</span> <span class="nav-text">8.模型的保存和加载</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-代码"><span class="nav-number">10.0.1.</span> <span class="nav-text">1.代码</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#var-list-自定要保持和还原的变量。他可以作为一个dict或者一个列表传进去"><span class="nav-number">10.0.1.0.0.1.</span> <span class="nav-text">var_list:自定要保持和还原的变量。他可以作为一个dict或者一个列表传进去</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#max-to-keep-制定要保留的最近检查点文件的最大数量，创建新的文件的时候会删除比较旧的文件，默认值5"><span class="nav-number">10.0.1.0.0.2.</span> <span class="nav-text">max_to_keep:制定要保留的最近检查点文件的最大数量，创建新的文件的时候会删除比较旧的文件，默认值5</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#“”-这个路径包含路径和文件名"><span class="nav-number">10.0.1.0.0.3.</span> <span class="nav-text">“” 这个路径包含路径和文件名</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-保存的文件格式"><span class="nav-number">10.0.2.</span> <span class="nav-text">2.保存的文件格式</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#9-队列机制"><span class="nav-number">11.</span> <span class="nav-text">9.队列机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-简述-1"><span class="nav-number">11.0.1.</span> <span class="nav-text">1.简述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-代码-1"><span class="nav-number">11.0.2.</span> <span class="nav-text">2.代码</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#capatity-队列的容量"><span class="nav-number">11.0.2.0.0.1.</span> <span class="nav-text">capatity 队列的容量</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#dtype队列存储的数据类型"><span class="nav-number">11.0.2.0.0.2.</span> <span class="nav-text">dtype队列存储的数据类型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#演示"><span class="nav-number">11.0.2.1.</span> <span class="nav-text">演示:</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#10-文件读取"><span class="nav-number">12.</span> <span class="nav-text">10.文件读取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-文件读取的过程"><span class="nav-number">12.0.1.</span> <span class="nav-text">1.文件读取的过程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-构造文件队列"><span class="nav-number">12.0.1.0.1.</span> <span class="nav-text">1.构造文件队列</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-构造阅读器"><span class="nav-number">12.0.1.0.2.</span> <span class="nav-text">2.构造阅读器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-对于每个样本进行解码"><span class="nav-number">12.0.1.0.3.</span> <span class="nav-text">3.对于每个样本进行解码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-批处理文件"><span class="nav-number">12.0.1.0.4.</span> <span class="nav-text">4.批处理文件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-文件读取的API介绍"><span class="nav-number">12.0.2.</span> <span class="nav-text">2.文件读取的API介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#构造文件队列"><span class="nav-number">12.0.2.0.1.</span> <span class="nav-text">构造文件队列</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#string-tensor-含有文件名的一阶张量（包含路径以及文件名的列表）"><span class="nav-number">12.0.2.0.1.1.</span> <span class="nav-text">string_tensor 含有文件名的一阶张量（包含路径以及文件名的列表）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#shuffle-是否乱序-默认乱序"><span class="nav-number">12.0.2.0.1.2.</span> <span class="nav-text">shuffle 是否乱序 默认乱序</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#num-epochs-过几遍数据，默认无限过数据"><span class="nav-number">12.0.2.0.1.3.</span> <span class="nav-text">num_epochs 过几遍数据，默认无限过数据</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#构造文件阅读器"><span class="nav-number">12.0.2.0.2.</span> <span class="nav-text">构造文件阅读器</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#文本文件阅读器，默认按行读取，（因为对于csv文件和文本文件来说，一行就是一个样本）"><span class="nav-number">12.0.2.0.2.1.</span> <span class="nav-text">文本文件阅读器，默认按行读取，（因为对于csv文件和文本文件来说，一行就是一个样本）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#return-返回阅读器实例"><span class="nav-number">12.0.2.0.2.2.</span> <span class="nav-text">return 返回阅读器实例</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#read-file-queue"><span class="nav-number">12.0.2.0.3.</span> <span class="nav-text">read(file_queue)</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#file-queue-从队列里面读取内容"><span class="nav-number">12.0.2.0.3.1.</span> <span class="nav-text">file_queue 从队列里面读取内容</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#二进制文件阅读器"><span class="nav-number">12.0.2.0.4.</span> <span class="nav-text">二进制文件阅读器</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#读取每个样本是按固定数量的字节读取的二进制文件"><span class="nav-number">12.0.2.0.4.1.</span> <span class="nav-text">读取每个样本是按固定数量的字节读取的二进制文件</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#record-bytes-整形，指定每次读取的字节数"><span class="nav-number">12.0.2.0.4.2.</span> <span class="nav-text">record_bytes:整形，指定每次读取的字节数</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#return-返回阅读器实例-1"><span class="nav-number">12.0.2.0.4.3.</span> <span class="nav-text">return 返回阅读器实例</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#图片文件阅读器"><span class="nav-number">12.0.2.0.5.</span> <span class="nav-text">图片文件阅读器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#每个样本进行解码"><span class="nav-number">12.0.2.0.6.</span> <span class="nav-text">每个样本进行解码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CSV文件解码"><span class="nav-number">12.0.2.0.7.</span> <span class="nav-text">CSV文件解码</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#value表示待解码的内容"><span class="nav-number">12.0.2.0.7.1.</span> <span class="nav-text">value表示待解码的内容</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#record-defaults-表示每个样本如何解码，并且缺失的时候的默认值-1-表示一个样本按整形解码，缺失的时候为1，-“None”-1-0-表示样本有两个第一个按string类型解码，缺失的时候是None-第二个按float类型解码，丢失按1-0处理"><span class="nav-number">12.0.2.0.7.2.</span> <span class="nav-text">record_defaults 表示每个样本如何解码，并且缺失的时候的默认值 [[1]]表示一个样本按整形解码，缺失的时候为1，[[“None”],[1.0]] 表示样本有两个第一个按string类型解码，缺失的时候是None,第二个按float类型解码，丢失按1.0处理</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#图片文件解码"><span class="nav-number">12.0.2.0.8.</span> <span class="nav-text">图片文件解码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#管道批处理文件"><span class="nav-number">12.0.2.0.9.</span> <span class="nav-text">管道批处理文件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-文件读取的简单演示"><span class="nav-number">12.0.3.</span> <span class="nav-text">3.文件读取的简单演示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#图片文件的读取简单演示"><span class="nav-number">12.0.4.</span> <span class="nav-text">图片文件的读取简单演示</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#11-交叉熵损失计算和softMax计算"><span class="nav-number">13.</span> <span class="nav-text">11.交叉熵损失计算和softMax计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-softMax计算"><span class="nav-number">13.0.1.</span> <span class="nav-text">1.softMax计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-交叉熵损失"><span class="nav-number">13.0.2.</span> <span class="nav-text">2.交叉熵损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-演示"><span class="nav-number">13.0.3.</span> <span class="nav-text">3.演示</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#12-卷积神经网络"><span class="nav-number">14.</span> <span class="nav-text">12.卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-概述"><span class="nav-number">14.0.1.</span> <span class="nav-text">1.概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-卷积神经网络分层"><span class="nav-number">14.0.2.</span> <span class="nav-text">2.卷积神经网络分层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积层—-gt-激活层—-gt-池化层—-gt-全连接层"><span class="nav-number">14.0.2.1.</span> <span class="nav-text">卷积层—&gt;激活层—&gt;池化层—&gt;全连接层</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#激活层："><span class="nav-number">14.0.2.1.1.</span> <span class="nav-text">激活层：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#池化层："><span class="nav-number">14.0.2.1.2.</span> <span class="nav-text">池化层：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-填充算法"><span class="nav-number">14.0.3.</span> <span class="nav-text">3.填充算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SAME："><span class="nav-number">14.0.3.1.</span> <span class="nav-text">SAME：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VALID："><span class="nav-number">14.0.3.2.</span> <span class="nav-text">VALID：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-API介绍"><span class="nav-number">14.0.4.</span> <span class="nav-text">4.API介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#input-输入的4-D张量，具有-batch-height-width-channel-类型为float32，或者float64-每批数-图片长度，图片宽度，图片通道数"><span class="nav-number">14.0.4.0.0.1.</span> <span class="nav-text">input: 输入的4-D张量，具有[batch,height,width,channel],类型为float32，或者float64([每批数,图片长度，图片宽度，图片通道数])</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#filter-指定过滤器4-D随机初始化的张量：-类型为float32或者float64-过滤器长度，过滤器宽度，输入的通道数，输出的通道数-，"><span class="nav-number">14.0.4.0.0.2.</span> <span class="nav-text">filter:指定过滤器4-D随机初始化的张量：,类型为float32或者float64([过滤器长度，过滤器宽度，输入的通道数，输出的通道数])，</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#——-注意需要传入4-D张量，而不是简单的填入维度"><span class="nav-number">14.0.4.0.0.3.</span> <span class="nav-text">——- 注意需要传入4-D张量，而不是简单的填入维度</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#strides-strides-1-stride-stride-1-步长（-1，长上步长，宽上步长，1-）"><span class="nav-number">14.0.4.0.0.4.</span> <span class="nav-text">strides:strides=[1,stride,stride,1],步长（[1，长上步长，宽上步长，1]）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#padding-”SAME”，“VALID”，使用的填充算法类型"><span class="nav-number">14.0.4.0.0.5.</span> <span class="nav-text">padding=:”SAME”，“VALID”，使用的填充算法类型.</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#features-卷积后加上偏置的结果"><span class="nav-number">14.0.4.0.0.6.</span> <span class="nav-text">features:卷积后加上偏置的结果</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#return-结果"><span class="nav-number">14.0.4.0.0.7.</span> <span class="nav-text">return :结果</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#value-4-D-tensor形状-batch-height-width-channels-，也就是激活函数处理后的结果"><span class="nav-number">14.0.4.0.0.8.</span> <span class="nav-text">value:4-D tensor形状[batch,height,width,channels]，也就是激活函数处理后的结果</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#ksize池化窗口大小，-1-ksize-ksize-1"><span class="nav-number">14.0.4.0.0.9.</span> <span class="nav-text">ksize池化窗口大小，[1,ksize,ksize,1]</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#步长大小-1-strides-strides-1"><span class="nav-number">14.0.4.0.0.10.</span> <span class="nav-text">步长大小,[1,strides,strides,1]</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#padding-”SAME”-”VALID”填充算法"><span class="nav-number">14.0.4.0.0.11.</span> <span class="nav-text">padding :”SAME”,”VALID”填充算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-演示"><span class="nav-number">14.0.5.</span> <span class="nav-text">5.演示</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#两层卷积神经网络识别手写数字"><span class="nav-number">14.0.5.0.1.</span> <span class="nav-text">两层卷积神经网络识别手写数字</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#卷积层"><span class="nav-number">14.0.5.0.1.1.</span> <span class="nav-text">卷积层</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#第一层卷积神经网络"><span class="nav-number">14.0.5.0.2.</span> <span class="nav-text">第一层卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#卷积层-1"><span class="nav-number">14.0.5.0.2.1.</span> <span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#激活层"><span class="nav-number">14.0.5.0.2.2.</span> <span class="nav-text">激活层</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#池化"><span class="nav-number">14.0.5.0.2.3.</span> <span class="nav-text">池化</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#全连接层"><span class="nav-number">14.0.5.0.3.</span> <span class="nav-text">全连接层</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yang295513</span>

  
</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
